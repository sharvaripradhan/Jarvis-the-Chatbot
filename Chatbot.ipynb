{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f875c6b",
   "metadata": {},
   "source": [
    "# Jarvis the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c862b4",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "### 1. Import corpus-- corpus data: training data for chatbot to learn\n",
    "### 2. Preprocess the data ---clean the data\n",
    "### 3. Text case handling ---convert data to one case\n",
    "### 4. Tokenization ---convert sentences into words\n",
    "### 5. Stemming ---finding similarities between words i.e get the root word\n",
    "### 6. Bag of words ---convert words to numbers by generating vector encoding\n",
    "### 7. One hot encoding ---pass the vectors to the ml algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428bc9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94440a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sharvari\n",
      "[nltk_data]     Pradhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sharvari\n",
      "[nltk_data]     Pradhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "f=open('chatbot_train_data.txt','r', errors='ignore')\n",
    "file=f.read()\n",
    "file=file.lower()\n",
    "nltk.download('punkt') #inbuilt tokenizer named punkt\n",
    "nltk.download('wordnet') #use wordnet dictionary\n",
    "sent_tokens=nltk.sent_tokenize(file) #converts file to sentences\n",
    "word_tokens=nltk.word_tokenize(file) #converts file to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5378dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small talk\\nfrom wikipedia, the free encyclopedia\\njump to navigationjump to search\\nthis article is about the type of discourse.',\n",
       " 'for other uses, see small talk (disambiguation).',\n",
       " '\"chit chat\" redirects here.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:3] #first 3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79f1315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small', 'talk', 'from', 'wikipedia', ',']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:5] #printing first 5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6c7d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "#wordnet dictionary included with nltk library. helps to remove punctuations\n",
    "\n",
    "lemmer=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(i) for i in tokens]\n",
    "\n",
    "remove_punctuations=dict((ord(punct),None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punctuations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e47093e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_inputs=['hey','hi','sup','whats up','hello']\n",
    "greet_response=['hey','hi','sup','whats up','hello','hi. how are you', 'hi nice to meet you']\n",
    "\n",
    "def greet(greeting):\n",
    "    for word in greeting.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26391c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response generation\n",
    "#tfidf--term frequency inverse document frequency--how many times a word is repeated. inverse doc-how rare is the word\n",
    "#cosine similarity--gives a normalized output and tells us how rare it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88398849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a51e7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    vector=TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf=vector.fit_transform(sent_tokens)\n",
    "    vals=cosine_similarity(tfidf[-1],tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat=vals.flatten()\n",
    "    flat.sort()\n",
    "    req=flat[-2]\n",
    "    \n",
    "    if req==0:\n",
    "        robo_response=robo_response+\"I dont understand this\" #invalid response\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response=robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb497ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am Jarvis. How can I help you today? If you wish to end the chat then just say bye.\n",
      "hi\n",
      "Jarvis: hi nice to meet you\n",
      "small talk\n",
      "Jarvis: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sharvari Pradhan\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for other uses, see small talk (disambiguation).\n",
      "references\n",
      "Jarvis: [26] [27]\n",
      "\n",
      "see also\n",
      "active listening\n",
      "cheap talk (game theory)\n",
      "contact call\n",
      "sociolinguistics\n",
      "transactional analysis\n",
      "phatic expression\n",
      "tritsch-tratsch-polka by johann strauss ii, from the german for \"chit-chat\"\n",
      "references\n",
      " \"dummies - learning made easy\".\n",
      "bye\n",
      "Jarvis: Goodbye! Take care!\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print('Hi, I am Jarvis. How can I help you today? If you wish to end the chat then just say bye.')\n",
    "while(flag==True): #keep running the bot untill user ends it or there is a pause of more than 1 min\n",
    "    user_response=input()\n",
    "    user_response=user_response.lower()\n",
    "    if user_response!='bye':\n",
    "        if user_response=='thanks' or user_response=='thank you':\n",
    "            flag=False\n",
    "            print('Jarvis: You are welcome')\n",
    "        else:\n",
    "            if greet(user_response) != None:\n",
    "                print('Jarvis: '+greet(user_response))\n",
    "            else:\n",
    "                sent_tokens.append(user_response)\n",
    "                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
    "                final_words=list(set(word_tokens))\n",
    "                print('Jarvis: ',end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print('Jarvis: Goodbye! Take care!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4fdbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
